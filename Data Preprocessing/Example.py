comments = [
    "Bu Ã¼rÃ¼n HARÄ°KA! Tavsiye ederim ğŸ˜",
    "berbat, iade etmek istiyorum.",
    "Ne gÃ¼zel paketlenmiÅŸti ama Ã¼rÃ¼n kÃ¶tÃ¼ Ã§Ä±ktÄ±.",
    "KESÄ°NLÄ°KLE bir daha alÄ±ÅŸveriÅŸ yapmam.",
    "hÄ±zlÄ± kargo teÅŸekkÃ¼rler ğŸ™"
]
# Kelimeleri kÃ¼Ã§Ã¼k harfe Ã§evirme
comments_lower = [sentence.lower() for sentence in comments]


# Noktalama iÅŸaretlerini silme

import string

comments_without_punctuation = [sentence.translate(str.maketrans("","",string.punctuation)) for sentence in comments_lower]


# Ã–zel karakterleri silme

import re

comments_without_special_ch = [re.sub(r"[^A-Za-z0-9ÅŸÅÄ±Ä°Ã§Ã‡Ã¶Ã–Ã¼ÃœÄŸÄ\s]","",sentence) for sentence in comments_without_punctuation]


# Kelimeleri tokenlarÄ±na ayÄ±rma
import nltk

nltk.download("punkt")

word_tokens = [nltk.word_tokenize(sentence) for sentence in comments_without_special_ch]


# Stop words'leri temizleme

import nltk
from nltk.corpus import stopwords

stop_words_tr = set(stopwords.words("turkish"))

comments_without_stop_words = [
    [word for word in sentence if word not in stop_words_tr]
    for sentence in word_tokens
]

# Stemming ve Lemmatization

import snowballstemmer


stemmer = snowballstemmer.stemmer('turkish')

stems = [
    [stemmer.stemWord(word) for word in sentence]
    for sentence in comments_without_stop_words
]

def clean_text(word):
    manual_lemma = {
        "Ã¼r": "Ã¼rÃ¼n",
        "tavsi": "tavsiye",
        "ia": "iade",
        "harik": "harika",
        "iste": "istemek",
        "paketlenmiÅŸ": "paket",
        "kesinlikl": "kesinlikle",
        "alÄ±ÅŸveriÅŸ": "alÄ±ÅŸveriÅŸ",
        "yapma": "yapmak",
        "teÅŸekkÃ¼r": "teÅŸekkÃ¼r"
    }
    return manual_lemma.get(word, word)  # kelime sÃ¶zlÃ¼kte varsa karÅŸÄ±lÄ±ÄŸÄ±nÄ±, yoksa kendisini dÃ¶ndÃ¼r


stemmed_fixed = [
    [clean_text(word) for word in sentence]
    for sentence in stems
]




